\documentclass[10pt]{beamer}

  % Math Packages
  \usepackage{algorithmic}
  \usepackage{amsmath}
  \usepackage{amsthm}
  \usepackage{mathtools}

  % Graphics Packages
  \usepackage[export]{adjustbox}
  \usepackage{graphicx}
  \usepackage{pgf}

  % Other packages
  \usepackage{color}
  \usepackage{csquotes}  % Adds ability to use display quotes
  \usepackage{listings}

  % Theme Settings
  \usetheme{metropolis}
  \usefonttheme{professionalfonts}

  % Special commands
  \DeclareMathOperator*{\argmin}{arg\,min}
  \DeclareMathOperator*{\argmax}{arg\,max}

\title{Introduction and Tools 1}
\author{Chase Coleman}
\institute{NYU Stern}
\date[]{\today}

\begin{document}

% Title Slide
\begin{frame}
  \thispagestyle{empty}
  \titlepage
\end{frame}


% --------------------------------------- %
% Introduction
% --------------------------------------- %
\section{Introduction}

\begin{frame} \frametitle{Welcome}

  Thanks for being here.

  Introduce yourself:
  \begin{itemize}
    \item Name
    \item Year of PhD
    \item Programming langauge you plan to use
    \item Answer the question you wish I asked you here
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Organizational Stuff}
  Next lecture will be Monday the 26th

  \vspace{0.5cm}

  All slides and practice questions will be online at \url{https://github.com/NYUEcon/CompEconWorkshop_2017}. We will try to provide answers to some of the practice questions we assign there as well.

  \vspace{0.5cm}

  Questions?

\end{frame}

% --------------------------------------- %
% Writing Good Code
% --------------------------------------- %
\section{Writing Good Code}

\begin{frame} \frametitle{An Economics Crisis}

  \begin{displayquote}
    I think we are in the middle of a simmering crisis in economics\dots I think the first-order issue with economists and programming languages is not (necessarily) performance, but (1) correctness and (2) software engineering\dots Have someone with a published numerical paper send you their code, and tell me if you can tell whether it implements the model or not\dots The software engineering problem in economics needs to be addressed with better training and helping people understand tools like version control, continuous integration, unit testing, the benefits of tested libraries, etc.
  \end{displayquote}

\end{frame}

\begin{frame} \frametitle{Intro to Software Engineering}
  What is ``Software Engineering?''

  \visible<1->{``The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software.''}

  What does this mean?

  \visible<2->{Write code to minimize the probability of having a bug and to ensure that others can understand your code without excessive effort\footnote{This can improve your citability}.
  \begin{displayquote}
    Always code as if the guy who ends up maintaining your code will be a violent pyschopath who knows where you live.
  \end{displayquote}
  }

\end{frame}

\begin{frame} \frametitle{Simple Rules to Live By}
  These are in no way exhaustive, but are a start
  \begin{itemize}
    \item Don't Repeat Yourself
    \begin{itemize}
      \item Use functions rather than writing things repeatedly
      \item Writing \lstinline{(c^(1 - gamma) - 1) / (1 - gamma)} repeatedly makes it likely you will eventually make a mistake
      \item It is a pain to make sure you change all instances if you decide to use a different utility function.
    \end{itemize}
    \item Write test cases to ensure that various functions you use do what you think they do.
    \item Use whitespace wisely (ALWAYS HAVE SPACE AFTER COMMA).
    \item Comment and document your code carefully.
  \end{itemize}
\end{frame}

% --------------------------------------- %
% Understanding the Basics
% --------------------------------------- %
\section{Understanding the Basics}

\begin{frame} \frametitle{Floating Point Numbers}
  Computer reads numbers differently than you.

  A 16 bit floating point number: $\underbrace{0}_{\text{Sign}} \underbrace{01101}_{\text{Exponent}} \underbrace{0101010101}_{\text{Mantissa}}$

  \begin{align*}
    0 01101 0101010101 &\rightarrow -1^{\text{Sign}} \left(1 + \sum_{n=1}^{10} \text{Mantissa}_n 2^{-n} \right) 2^{\text{Exponent - Bias}} \\
                       &\rightarrow 1 (1.3330078125) 2^{13 - 15} = 0.33325
  \end{align*}

  Occasionally useful to understand this fact.

  \visible<2->{For example, since computer arithmetic is not necessarily associative or commutative\footnote{See \url{http://www.walkingrandomly.com/?p=5380} for discussion} for floating points (because of accuracy limitations) checking exact equality between floating points is a \textit{bad} idea\dots}
  % Example: 10 * 0.07 vs 10/5000 * (5000 * 0.07)
  %           0.1 + (0.2 + 0.3) == (0.1 + 0.2) + 0.3 => False

\end{frame}

\begin{frame} \frametitle{Ill Conditioned Matrices}

Consider $A = \begin{bmatrix} 2 & 6 \\ 2 & 6.00001 \end{bmatrix}$, $b_1 = \begin{bmatrix} 8 \\ 8.00001 \end{bmatrix}$, and $b_2 = \begin{bmatrix} 8 \\ 8.00002 \end{bmatrix}$

  \begin{itemize}
    \item Solution to $A x = b_1$ is $\begin{bmatrix} 1 & 1 \end{bmatrix}$
    \item Solution to $A x = b_2$ is $\begin{bmatrix} 2 & -2 \end{bmatrix}$
  \end{itemize}

  An ill conditioned matrix is one that is close to singular. This causes, for the equation $A x = b$, that small changes in $b$ lead to large changes in $x$.

\end{frame}

\begin{frame} \frametitle{Dealing with Ill Conditioned Matrices}
  Can check whether a matrix is ill conditioned by looking at its condition number:

  $$\kappa(A) = \sigma_{\text{max}}(A) / \sigma_{\text{min}}(A)$$

  If a matrix is actually ill-conditioned ($\kappa(A)$ large), there is not much you can do except try to make sure $b$ is calculated as precisely as possible.
\end{frame}

\begin{frame}[label=ColRowMajor] \frametitle{Column vs Row Major}

  Matrices (and higher dimensional arrays) are stored as sequential elements of memory. The order in which they are stored determines whether a language is column or row major.

  For example, consider the following matrix: $\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$
  \begin{itemize}
    \item A row major language would store this as: \lstinline{1, 2, 3, 4, 5, 6}
    \item A column major language would store this as: \lstinline{1, 4, 2, 5, 3, 6}
  \end{itemize}

  This has performance implications (More info \hyperlink{ColRowMajor_Supp}{\beamerbutton{here}})
\end{frame}

\begin{frame} \frametitle{Curse of Dimensionality}
  Often need to approximate functions $f: \mathbb{R}^d \rightarrow \mathbb{R}$

  If want to use $n$ points to approximate function in each dimension using a tensor grid of points then have a total of $n^d$ points.

  \begin{figure}
    \centering
    \scalebox{0.5}{\input{images/curseofdimensionality.pgf}}
  \end{figure}

\end{frame}

\begin{frame} \frametitle{Curse of Dimensionality}

  Most obvious way to slow down the curse of dimensionality is to move away from tensor grids!

  \begin{figure}
    \centering
    \scalebox{0.5}{\input{images/smolyakrandomtensor.pgf}}
  \end{figure}

\end{frame}

\begin{frame} \frametitle{Vectorization}

  Sometimes useful to recognize that loops can be avoided by using vectorized operations.

  For example, imagine a Markov chain, $(S_t, P, \pi_0)$, for endowments.

  $$V(s_t) = u(s_t) + \beta E \left[ V(s_{t+1}) | s_t \right]$$

  Recognize that $\beta E[V(s_{t+1}) | s_t]$ is just $\beta P \vec{V}$ thus $\vec{V} = (I - \beta P) u(\vec{s})$

\end{frame}


% --------------------------------------- %
% Root Finding and Optimization
% --------------------------------------- %
\section{Root Finding and Optimization}

\subsection{Root Finding}

\begin{frame} \frametitle{Goal of Root Finding}

  Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$, find $x \in \mathbb{R}^n$ such that $f(x) = \vec{0}$

  Whenever we search for prices in a general equilibrium model, we are seeking to clear markets \textemdash{} i.e.\ find prices such that the excess demand functions are zero.

  In algorithms presented, the stopping point will be determined by how close $x_{k+1}$ and $x_k$ are, but the algorithm is only a success if ultimately $f(x_{k+1}) \approx 0$. For space constraints, I have left this function convergence check out of the algorithm descriptions.

\end{frame}

\begin{frame} \frametitle{Bisection: Simplest Root Finding Algorithm}

  Consider a continuous function $f : \mathbb{R} \rightarrow \mathbb{R}$ and $a, b \in \mathbb{R}$ such that $f(a) f(b) < 0$. Then Intermediate Value Theorem states, $\exists c \in (a, b)$ such that $f(c)  = 0$.

  \vspace{0.3cm}

  \begin{algorithmic}
    \REQUIRE $f(a) f(b) < 0$

    \STATE $fc \leftarrow 10$

    \WHILE{$|b - a| > tol$}
      \STATE $c \leftarrow (a+b)/2$
      \STATE $fc \leftarrow f(c)$

      \IF{$fa*fc < 0$}
        \STATE $b \leftarrow c$
      \ELSE
        \STATE $a \leftarrow c$
      \ENDIF
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Bisection}
  Simplest root finding algorithm
  \begin{itemize}
    \item Pros: Relatively fast, simple, guaranteed to find a solution given certain conditions
    \item Cons: Uses little info about function, not natural to extend to higher than 1 dimension
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Newton's Method}

  Natural follow up to Bisection is Newton's Method which uses information about derivatives by linearizing and find the zero of the linearized version.

  $$f(x_k) + f'(x_k) (x_{k+1} - x_{k}) = 0$$

  \vspace{0.5cm}

  \begin{algorithmic}
    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
      \STATE $x_{k+1} \rightarrow x_k - f(x_k)/f'(x_k)$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Newton's Method}
  \begin{itemize}
    \item Pros: Even faster than bisection, simple, uses information about derivative
    \item Cons: Does not always find a solution, requires ability to compute derivative
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Secant Method}
  Secand method is related to Newton's Method, but, rather than use the analytical derivative, it approximates $f'(x)$ by taking secant line between $f(x_{k-1})$ and $f(x_k)$

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE $x_{k+1} \rightarrow x_k - \frac{f(x_k) (x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Secant Method}
  \begin{itemize}
    \item Pros: Similar in speed to Newton's method \textemdash{} The relative speed depends on how much time evaluating the derivative takes vs possibly slower convergence because not using actual derivative, sometimes useful not to need exact derivative
    \item Cons: Not guaranteed to converge, doesn't use analytical derivative
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Multi-dimesional Newton}

  Use Jacobian rather than derivative

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE $A_k = J(x^k)$
    \STATE Solve $A_k s_k = - f(x^k)$ for $s_k$
    \STATE $x_{k+1} \rightarrow x_k + s_k$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Multi-dimesional Secant (aka Broyden)}

  This is a multi-dimensional version of Secant method. We must determine how to approximate Jacobian.

  One way\footnote{This ensure that $A_{k+1} q = A_k q$ whenever $q^T s_k = 0$ (predicted change in directions orthogonal to $s_k$ with new Jacobian matrix are equal to predicted change in that direction with old Jacobian matrix)}
: $A_{k+1} = A_k + \frac{(y_k - A_k s_k) s_k^T}{s_k^T s_k}$

  \vspace{0.3cm}

  \begin{algorithmic}

    \STATE $A_0 = I$

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE Solve $A_k s_k = - f(x^k)$ for $s_k$
    \STATE $x_{k+1} \rightarrow x_k + s_k$
    \STATE $y_k = f(x^{k+1}) - f(x^k)$
    \STATE $A_{k+1} = A_k + \frac{(y_k - A_k s_k) s_k^T}{s_k^T s_k}$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Gauss-Jacobi and Gauss-Seidel}

  Both of these algorithms simplify a $n$-dimensional problem to a 1-dimensional problem. Solve for $x_i$ such that $f(x_i, x_{-i}) = 0$. However, the approach is slightly different.

  \begin{itemize}
    \item \textbf{Gauss-Jacobi}: Find $x^{k+1}$ such that its elements satisfy following equations
    \begin{align*}
      f^1(x_1^{k+1}, x_2^k, \dots, x_n^k) &= 0 \\
      f^2(x_1^{k}, x_2^{k+1}, \dots, x_n^k) &= 0 \\
      \dots
    \end{align*}
    \item \textbf{Gauss-Seidel}: Find $x^{k+1}$ such that its elements satisfy following equations
    \begin{align*}
      f^1(x_1^{k+1}, x_2^k, \dots, x_n^k) &= 0 \\
      f^2(x_1^{k+1}, x_2^{k+1}, \dots, x_n^k) &= 0 \\
      \dots
    \end{align*}
  \end{itemize}

\end{frame}

\subsection{Optimization}

\begin{frame} \frametitle{Goal of Optimization}

  Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, find $x^* \in \mathbb{R}^n$ such that $x^* \in \argmin_x f(x)$

  \vspace{0.3cm}

  Economic models typically have various agents who are each solving a maximization problem \textemdash{} For example, consumers choose consumption and savings in order to maximize their value today (flow + continuation).

  \vspace{0.3cm}

  {\color{red} WARNING: All methods shown today are about converging to local extrema\dots This is often not problematic in economic problems because of ``nice'' properties, but not universally true.}

\end{frame}

\begin{frame} \frametitle{Grid Search}
  ``Premature optimization is the root of all evil'' \textemdash{Donald Knuth}

  \vspace{0.5cm}

  A very simple way to get an idea of where the minimum of a function is, is to simply compare on a discrete number of points.

  \vspace{0.5cm}

  This approach is often more effective than people give it credit for.

\end{frame}

\begin{frame} \frametitle{Golden-Section Search}

  Member of a class of methods known as bracketing methods.

  \vspace{0.5cm}

  \begin{algorithmic}

    \REQUIRE $a, b$

    \STATE $c \leftarrow b - (b-a)/\phi$
    \STATE $d \leftarrow a + (b-a)/\phi$

    \WHILE{$|c - d| > tol_x$}

    \IF{$f(c) < f(d)$}
      \STATE $(b, fb) \leftarrow (d, fd)$
    \ELSE
      \STATE $(a, fa) \leftarrow (c, fc)$
    \ENDIF

    \STATE $c \leftarrow b - (b-a)/\phi$
    \STATE $d \leftarrow a + (b-a)/\phi$

    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Nelder-Mead (Simplex Method)}

  Member of a class of methods known as polytope methods. The general idea\footnote{For a full description refer to \url{http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/chase_nelder_mead.ipynb}} is that you build a simplex and expand, shrink, move, and rotate the simplex until the distance between the points of the simplex are very small.

\end{frame}

\begin{frame} \frametitle{Gradient Descent}

  The idea of gradient descent is to move in the steepest direction ``down the hill.'' Very effective in first few iterations, but can be slow as it gets near the minimum\dots

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x^{k+1} - x^{k}| > tol_x$}
      \STATE $\gamma^k = \frac{(x^k - x^{k-1})^T (\nabla f(x^k) - \nabla f(x^{k-1}))}{||\nabla f(x^k) - \nabla f(x^{k-1})||^2}$
      \STATE $x^{k+1} \leftarrow x^k - \gamma^k \nabla f(x^k)$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Newton's Method (Optimization)}

  Like finding the root of $f'(x)$\dots\ Similar in idea to gradient descent, but requires/uses additional information (Hessian) which overcomes the problem of when gradient descent gets slow.

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE Compute gradient, $\nabla f (x^k)$
    \STATE Compute Hessian, $H(x^k)$
    \STATE Solve for $s^k$ in $H(x^k) s^k = -(\nabla f(x^k))^T $
    \STATE $x^{k+1} \leftarrow x^k + s^k$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{BFGS Method}

  Similar to the ideas to gradient descent and Newton's method, but want to get bonuses of having information about Hessian without actually computing it. Approximation of Hessian is $B_k$.

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE $p_k$ as solution of $B_k p_k = - \nabla f(x_k)$
    \STATE Perform line search to find good stepsize $\alpha_k$
    \STATE $s_k \leftarrow \alpha_k p_k$
    \STATE $x^{k+1} \leftarrow x^k + s_k$
    \STATE $y_k = \nabla f(x^{k+1}) - \nabla f(x^k)$
    \STATE $B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$
    \ENDWHILE

  \end{algorithmic}

\end{frame}

\begin{frame} \frametitle{Goal of Constrained Optimization}

  Given functions $f: \mathbb{R}^n \rightarrow \mathbb{R}$, $g: \mathbb{R}^n \rightarrow \mathbb{R}^m$, and $h: \mathbb{R}^n \rightarrow \mathbb{R}^p$ solve
  \begin{align*}
    x^* &= \argmin_x f(x) \\
        &\text{subject to } \\
        g(x^*) &= 0 \\
        h(x^*) &\leq 0
    \end{align*}

\end{frame}

\begin{frame} \frametitle{Constrained: Penalty Function Methods}

  One way to solve a constrained optimization problem is by just building the Lagrangian and using the theory in the Karush-Kuhn-Tucker conditions.

  \vspace{0.5cm}

  \begin{algorithmic}

    \WHILE{$|x_{k+1} - x_{k}| > tol_x$}
    \STATE Solve a particular unconstrained problem to get $x_{k+1}$
    $$x_{k+1} \leftarrow \argmin_x f(x) + \frac{1}{2} P_k \left( \sum_i g_i(x)^2 + \sum_j \max \{0, h_j(x) \}^2 \right)$$
    \STATE $P_{k+1} = \chi P_{k}$
    \ENDWHILE

  \end{algorithmic}

  \vspace{0.5cm}

  Why increase $P$ over time? (If too large then ill-conditioning can hurt accuracy)

\end{frame}

\begin{frame} \frametitle{Constrained: Sequential Quadratic}

  Leverages the fact that problems with quadratic objectives and linear constraints are ``easy'' to solve. Instead of solving original problem, solve a sequence of Quadratic-Linear problems

  \begin{align*}
    \min_s &(x^k - s)^T \mathcal{L}_xx(x^k, \lambda^k, \mu^k) (x^k - s) \\
    \text{s.t. } &g_x(x^k) (x^k - s) = 0 \\
    &h_x(x) (x^k - s) \leq 0
  \end{align*}

\end{frame}

\appendix

\begin{frame}[label=ColRowMajor_Supp] \frametitle{Column vs Row Major: Supplement}
  Computers have different layers of memory storage. In order to apply operations, data must be moved to a small piece of memory called L1 memory. L1 memory lives directly on the processor and is very fast.

  As computers have evolved, they have gotten better at ``guessing'' what data you will need next. Instead of moving just a single element of an array, the processor will retrieve a small block of consecutive elements.

  As you iterate along an array, this means the computer will need to spend less time passing memory between RAM and L1 and can spend more time doing actual operations.

  Additional info: \href{https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips}{online}

  \hyperlink{ColRowMajor}{\beamerbutton{Back}} to Column vs Row Major
\end{frame}

\begin{frame} \frametitle{References}
  \begin{itemize}
    \item Numerical Methods in Economics by Kenneth L Judd
  \end{itemize}
\end{frame}

\end{document}

